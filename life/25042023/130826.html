<!DOCTYPE html><html lang="zh-CN"class="no-js"><head><meta http-equiv="Content-Type"content="text/html; charset=utf-8"/><meta name="viewport"content="initial-scale=1.0,user-scalable=no,minimal-ui"/><meta http-equiv="X-UA-Compatible"content="IE=edge"><meta name="keywords"content="生成,高精,细节,新方法,AligNeRF,解决,NeRF,"/><meta name="description"content="大家好，今天小编给大家整理了下面相关知识点，如果碰巧解决您的问题，别忘了关注本站哦，现在一起来看看吧 机器之心报道 机器之心编辑部 与当前最先进的 NeRF 模型相比，AligNe"/><title>生成高精细节，新方法 AligNeRF 解决 NeRF 对齐问题_DY生活网-分享生活经验知识</title><link rel='stylesheet'id='puma-css'href='/skin/css/bundle.css'type='text/css'media='screen'/><link rel='stylesheet'id='fancyratings-css'href='/skin/css/style.css'type='text/css'media='all'/><script type='text/javascript'src='/skin/js/jquery.js'></script><script type='text/javascript'src='/skin/js/jquery-migrate.min.js'></script><script type='text/javascript'src='/skin/js/m.js'></script></head><body class="post-template-default single single-post postid-2436 single-format-standard"><div class="surface-content"><header class="site-header u-textAlignCenter container"><div class="header-inner"><h1 class="site-title"><a href="/"title="">DY生活网</a></h1><p class="site-description">分享生活经验百科全书，是您互联网上的实用生活指南</p><div class="social-links"><span class="social-link"><a href="/sitemap.xml"target="_blank"><span class="icon-rss"></span></a></span><span class="social-link"><a href="javascript:;"class="opensearch"><span class="icon-search"></span></a></span></div><div class=""><form name="formsearch"role="search"method="get"class="search-form"action="/plus/search.php"><input type="hidden"name="kwtype"value="0"/><label><span class="screen-reader-text">搜索：</span><input type="search"class="search-field"placeholder="搜索&hellip;"value=""name="q"id"q"/></label><input type="submit"class="search-submit"value="搜索"/></form></div></div></header><nav class="topNav u-textAlignCenter container"><div class="layoutSingleColumn"><ul id="menu-%e9%a1%b6%e9%83%a8%e5%af%bc%e8%88%aa"class="topNav-items"><li class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item"><a href="/baike/">百科知识</a></li><li class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item"><a href="/meishi/">美食常识</a></li><li class='menu-item menu-item-type-taxonomy menu-item-object-category current-menu-item'><a href='/life/'>生活常识</a> </li><li class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item"><a href="/shuma/">数码百科</a></li><li class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item"><a href="/huahui/">花卉百科</a></li><li class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item"><a href="/qiche/">汽车知识</a></li><li class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item"><a href="/jiaoyu/">教育常识</a></li><li class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item"><a href="/t.html">男人世界</a></li></ul></div></nav><main class="main-content container"><section class="section-body"><header class="section-header u-textAlignCenter"><h2 class="grap--h2">生成高精细节，新方法 AligNeRF 解决 NeRF 对齐问题</h2><div class="block-postMetaWrap"><time>更新时间：2023-04-25 20:02</time></div><div class="post--keywords"itemprop="keywords"> </div></header><div class="grap"><p>大家好，今天小编给大家整理了下面相关知识点，如果碰巧解决您的问题，别忘了关注本站哦，现在一起来看看吧</p><p style="text-align: center"><img src="/pic/img3089.jpg"/></p>
			<p>机器之心报道</p><p><strong>机器之心编辑部</strong></p><p>与当前最先进的 NeRF 模型相比，AligNeRF 可以恢复更多的高频细节。</p><p>虽然 NeRF 能够用不同视角的视图中渲染复杂的 3D 场景，但很少有人致力于探索其在高分辨率设置中的局限性。具体来说，现有的基于 NeRF 的方法在重建高分辨率的真实场景时面临着一些限制，包括大量的参数、未对齐的输入数据和过于平滑的细节。</p><p>在 UC Austin、谷歌、香港中文大学等机构提出的一项新研究中，作者找到了相应的解决方案：1 ) 将多层感知器（MLP）与卷积层相结合，可以编码更多的邻域信息，同时减少参数总数；2 ) 一种新的训练策略来解决由移动物体或摄像机空间坐标校准误差引起的偏移；3 ) 高频感知损失。作者的方法几乎没有引入明显的训练和测试成本，而在不同数据集上的实验表明，与基本的 NeRF 模型相比，该工作可以恢复更多的高频细节。</p><p>论文地址：https://arxiv.org/abs/2211.09682</p><p>项目地址：https://yifanjiang19.github.io/alignerf</p><p><strong>简介</strong></p><p>神经辐射场（NeRF）及其变体，最近在从图像中学习几何三维表示方面表现出了令人印象深刻的性能。由此产生的高质量的场景表示创造了沉浸式的新视图合成体验，与复杂的几何形状和视图依赖的外观。自 NeRF 诞生以来，人们已经做了大量的工作来提高其质量和效率，使其能够从「野外」捕获的数据或有限数量的输入和跨多个场景的泛化中进行重建。</p><p>在本文中，作者以高分辨率的图像数据作为输入，在高保真设置下进行训练神经辐射场的初步研究。这就带来了几个主要的挑战：首先，使用高分辨率训练图像的主要挑战在于编码所有高频细节需要更多的参数，这会导致更长的训练时间和更高的内存成本。</p><p>在新模型中，作者在训练期间渲染图像补丁块。这使作者能够进一步解决渲染的补丁和 groud truth 之间的失调，这通常是由微小的相机姿态错误或被拍摄物体的轻微移动造成的。首先，作者分析了错位如何通过利用训练后渲染出的图像帧和相应的 groud truth 之间的估计光流来影响推理图像质量。作者分析并讨论了以前的错位感知损失的局限性，并为根据作者的任务目标提出了一种新的对齐策略。作者设计了一种新的频率感知损失，它进一步提高了训练集测试集的渲染质量，并且没有额外的开销。因此，AligNeRF 在高分辨率 3D 重建任务中的性能大大优于目前最好的方法。</p><p>综上所述，作者的贡献如下：</p><p>分析并显示了高分辨率训练数据的错位导致的性能下降。</p><p>一种新的卷积网络辅助架构，可以以少量的额外成本提高渲染图像的质量。</p><p>一种新的补丁对齐损失，使 NeRF 对相机姿态误差和微妙的物体运动更鲁棒，结合基于补丁的损失，以提高高频细节。</p><p><strong>方法</strong></p><p>AligNeRF 是一个易于插入的组件，适用于任何类似 NeRF 的模型，包括点采样方法和基于截锥体的方法。AligNeRF 使用分阶段训练：从初始的「正常」预训练阶段开始，然后是对齐感知微调阶段。作者选择 mip-NeRF 360 作为工作的基线，因为它是用于复杂无界现实世界场景的最先进的 NeRF 方法。接下来，先介绍作者的卷积增强架构，随后是错位感知训练过程和高频损失。</p><p>作者先是探索如何有效地编码局部归纳先验知识以用于基于坐标的 NeRF 表示 。类似 NeRF 的模型通常会构建一个坐标到值的映射函数，随机采样一批光线以优化其参数，并且中间没有任何优化操作。为此作者把从随机采样切换到基于补丁的采样（作者在实验中使用 32 × 32 补丁），这种基于补丁块的采样策略允许作者在每次迭代期间收集一个小的局部图像区域，从而在渲染每个像素时利用 2D 局部邻域信息。</p><p>首先将 MLP 中最后一层的输出通道数从 3 更改为更大的 N， 这有助于在每个采样光线中收集更丰富的表示。接下来在体积渲染后，添加一个简单的 3 层卷积网络，具有 ReLU 激活和 3 × 3 个内核。在该网络的末端，作者使用前馈感知器层将表示从特征空间转换为 RGB 空间。因此，每个像素的渲染过程不仅依赖于沿该方向的单个射线或圆锥射线区域，还依赖于其相邻区域，这有助于产生更好的纹理细节。</p><p>NeRF 通过渲染函数映射 3D 点到场景属性的关系来建模。在此框架下，训练样本相机位姿的准确性对于 NeRF 训练至关重要，否则从不同视点观察同一 3D 点的光线可能不会汇聚到空间中的同一位置 NeRF 通过在非常短的时间跨度内捕获图像（以防止场景运动和光照变化）并采用 COLMAP 来计算相机参数来解决这个问题。1 ) 地面实况相机姿势与来自 COLMAP 的相机姿势之间存在差距，该数据准备的工作流程大部分是可靠的，正如之前的工作所指出的那样：2）在不受控制的室外场景中，通常很难避免带有摇曳植物和其他非刚性静止物体的图像，这进一步损害了 COLMAP 的性能。</p><p>在高分辨率重建设置中，由相机姿势和移动物体引起的错位问题可能会进一步放大，因为像素空间错位与分辨率呈线性关系。为了解决这个问题，作者提出了一种对齐感知训练策略，可以用来改进渲染图像的质量。</p><p>尽管纹理扭曲，作者观察到 NeRF 仍然从未对齐的图像中学习粗糙结构。利用这一点，作者提出了对齐的 groud truth 和渲染块之间的 Loss。设置了一个基于欧氏距离的正则化项作为对该搜索空间的惩罚，最终的损失函数为：</p><p>均方误差 ( MSE ) 损失通常用于监督 NeRF 训练，但 MSE 经常导致输出图像模糊。鉴于作者的补丁采样策略，作者可以采用感知损失，更好地保留高频细节。作者首先尝试使用预训练 VGG 特征的 L2 损失。然而，与其他图像恢复任务类似，作者发现感知损失会产生更多的高频细节，但有时会扭曲物体的实际纹理。因此，作者修改了 Johnson 等人提出的原始感知损失，仅使用最大池化之前第一个块的输出：</p><p>AligNeRF 与之前工作的主要区别是从每像素 MSE 损失切换到基于块的 MSE 损失（考虑未对齐）和浅层 VGG 特征空间损失的组合，以改善高频细节：</p><p><strong>实验效果</strong></p><p><strong>定量分析</strong></p><p>为了进行公平的比较，作者将所提出的 AligNeRF 是基于 mip-NeRF 360 的方法上，并注意不通过作者的分阶段性的训练（训练前的 + 微调）来增加训练时间。由于这个实验使用了更高分辨率的图像，也可以看到作者增加训练时间的 4 倍以保持相同的训练期数量的结果。如下表所示，NeRF 和 mip-NeRF 的性能较差，因为它们不是为 360 度无界场景设计的。增加 mip-NeRF 的参数会有很小的改善，但会使训练时间更长。</p><p>作者提出的方法在两组中都优于 baseline 方法，而且并没有引入显著的训练开销。下表是与一些较为流行的方法的比较，在这些方法中，作者的方法在三个指标中展示了最好的性能，而且在低分辨率图像上的错位问题要比其他的方法要轻得多。</p><p><strong>定性分析</strong></p><p>首先，作者训练一个具有默认参数（1024 个通道）的 mip-NeRF 360 模型。但是仅仅是简单的 baseline 模型产生了模糊的图像，并且估计的光流包含了扭曲区域（第一列）中的伪影。接下来，作者将 mip-NeRF 360 网络参数增加 4 倍，但是这仅仅会略微提高结果的视觉质量，作者也应用迭代对齐策略来改进这个获得了更好的模型的结果。与在错位数据（前两列）上训练的模型相比，使用再生数据训练的模型恢复了更清晰的细节。这一观察结果表明，目前基于 NeRF 模型受到不对准相机位姿的训练样本的强烈影响。</p><p><strong>总结及未来展望</strong></p><p>在这项工作中，作者对高分辨率数据上的训练神经辐射场进行了初步研究。他们提出了一种有效的对齐感知训练策 AligNeRF 可以提高 NeRF 的性能。作者还定量和定性地分析了错位数据和通过使用光流估计重新生成对齐数据带来的性能下降。这一分析进一步帮助我们理解目前将 NeRF 扩展到更高分辨率的瓶颈问题。我们可以观察到，可以通过大幅增加参数的数量和进一步增加训练时间来进一步改进 NeRF，如何缩小这一差距是未来的研究方向。</p><p> THE END </p><p>转载请联系本公众号获得授权</p><p>投稿或寻求报道：content@jiqizhixin.com</p>		
		<p>这篇文章已经分享到这里了，希望对大家有所帮助。如果信息有误，请联系小编进行更正。</p></div><div class="post--keywords"itemprop="keywords"> </div><nav class="navigation post-navigation"role="navigation"><h2 class="screen-reader-text">文章导航</h2><div class="nav-links"><div class="nav-previous">上一篇：<a href='/life/25042023/130824.html'>花小钱办大事 中柏 EZbook S5 max 评测</a> </div><div class="nav-next">下一篇：<a href='/life/25042023/130827.html'>男生肖牛和什么生肖配（和属猪女是非常般配的一对）</a> </div></div></nav><div class="postFooterinfo u-textAlignCenter"></div><style>.button{background-color:#4CAF50;border:none;color:white;padding:5px 20px;text-align:center;text-decoration:none;display:inline-block;font-size:16px;margin:4px 2px;cursor:pointer}.button2{background-color:#008CBA}.button3{background-color:#f44336}</style><div style="font-family:verdana;padding:8px;border-radius:10px;border:5px solid #cdcecd"><p style="font-weight: bold;color: red">真正男人的世界↓警告：未满18禁入</p><button class="button"><a href="/t.html"target="_blank"><font color="#FFFFFF">线路一</font></a></button><button class="button button2"><a href="/t.html"target="_blank"><font color="#FFFFFF">线路二</font></a></button><button class="button button3"><a href="/t.html"target="_blank"><font color="#FFFFFF">线路三</font></a></button></div></section></main><footer class="site-footer u-textAlignCenter container">Copyright <a href="/">DOUYINDV</a>. <a href="http://www.miitbeian.gov.cn/" target="_blank">粤ICP备190376918号</a></footer><script type='text/javascript' src='/skin/js/d.js'></script></div></body></html>